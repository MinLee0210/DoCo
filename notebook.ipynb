{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LanChain lib\n",
    "from langchain.chains import LLMCheckerChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, PyPDFLoader, DirectoryLoader\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "from utils import to_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", \n",
    "                             google_api_key=GEMINI_API_KEY, \n",
    "                             temperature=0.7, \n",
    "                             top_p=0.85, \n",
    "                             top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMCheckerChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What type of mammal lays the biggest eggs?',\n",
       " 'result': 'The question cannot be answered based on the given assertions and checks. The assertions and checks do not provide any information about the size of eggs laid by monotremes.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What type of mammal lays the biggest eggs?\"\n",
    "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
    "checker_chain.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Article: {text}\n",
    "You will generate increasingly concise, entity-dense summaries of the\n",
    "above article.\n",
    "Repeat the following 2 steps 5 times.\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article\n",
    "which are missing from the previously generated summary.\n",
    "Step 2. Write a new, denser summary of identical length which covers every\n",
    "entity and detail from the previous summary plus the missing entities.\n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story,\n",
    "- specific yet concise (5 words or fewer),\n",
    "- novel (not in the previous summary),\n",
    "- faithful (present in the article),\n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "- The first summary should be long (4-5 sentences, ~80 words) yet highly\n",
    "non-specific, containing little information beyond the entities marked\n",
    "as missing. Use overly verbose language and fillers (e.g., \"this article\n",
    "discusses\") to reach ~80 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and\n",
    "make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative\n",
    "phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained,\n",
    "i.e., easily understood without the article.\n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made,\n",
    "add fewer new entities.\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose\n",
    "keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> ```json\n",
       "> [\n",
       ">   {\n",
       ">     \"Missing_Entities\": \"hallucination;LLMs\",\n",
       ">     \"Denser_Summary\": \"This article discusses the issue of hallucination in LLMs, which refers to the generation of unfaithful or nonsensical text compared to the input. Hallucinations can spread misinformation, rumors, and deceptive content, posing threats to society such as distrust in science, polarization, and democratic processes.\"\n",
       ">   },\n",
       ">   {\n",
       ">     \"Missing_Entities\": \"fact-checking;verification\",\n",
       ">     \"Denser_Summary\": \"One technique to address hallucinations is automatic fact-checking, which involves verifying claims made by LLMs against evidence from external sources. Fact-checking allows for catching incorrect or unverified statements and consists of three main stages: claim detection, evidence retrieval, and verdict prediction.\"\n",
       ">   },\n",
       ">   {\n",
       ">     \"Missing_Entities\": \"pre-trained LLMs;world knowledge\",\n",
       ">     \"Denser_Summary\": \"Pre-trained LLMs contain extensive world knowledge from their training data, which can be prompted for facts. By grounding claims in data, fact-checking makes LLMs more reliable.\"\n",
       ">   },\n",
       ">   {\n",
       ">     \"Missing_Entities\": \"LangChain;LLMCheckerChain\",\n",
       ">     \"Denser_Summary\": \"In LangChain, we have a chain available for fact-checking with prompt chaining, where a model actively questions the assumptions that went into a statement. In this self-checking chain, LLMCheckerChain, the model is prompted sequentially to make the assumptions explicit and then check them one by one.\"\n",
       ">   },\n",
       ">   {\n",
       ">     \"Missing_Entities\": \"question-answering tasks;domain datasets\",\n",
       ">     \"Denser_Summary\": \"Fact-checking approaches involve decomposing claims into smaller checkable queries, which can be formulated as question-answering tasks. Tools designed for searching domain datasets can assist fact-checkers in finding evidence effectively.\"\n",
       ">   }\n",
       "> ]\n",
       "> ```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "text = \"\"\"\n",
    "As discussed in previous chapters, hallucination in LLMs refers to the generated text being unfaithful or nonsensical compared to the input. It contrasts with faithfulness, where outputs stay\n",
    "consistent with the source. Hallucinations can spread misinformation like disinformation, rumors,\n",
    "and deceptive content. This poses threats to society, including distrust in science, polarization,\n",
    "and democratic processes.\n",
    "Journalism and archival studies have researched misinformation extensively. Fact-checking initiatives provide training and resources to journalists and independent checkers, allowing expert\n",
    "verification at scale. Addressing false claims is crucial to preserving information integrity and\n",
    "combatting detrimental societal impacts.\n",
    "One technique to address hallucinations is automatic fact-checking – verifying claims made by\n",
    "LLMs against evidence from external sources. This allows for catching incorrect or unverified\n",
    "statements.\n",
    "Fact-checking involves three main stages:\n",
    "1. Claim detection: Identify parts needing verification\n",
    "2. Evidence retrieval: Find sources supporting or refuting the claim\n",
    "3. Verdict prediction: Assess claim veracity based on evidence\n",
    "Alternative terms for the last two stages are justification production and verdict prediction.\n",
    "We can see the general idea of these three stages illustrated in the following diagram (source –\n",
    "https://github.com/Cartus/Automated-Fact-Checking-Resources by Zhijiang Guo):\n",
    "Figure 4.1: Automatic fact-checking pipeline in three stages\n",
    "Chapter 4 101\n",
    "Pre-trained LLMs contain extensive world knowledge that can be prompted for facts. Additionally,\n",
    "external tools can search knowledge bases, Wikipedia, textbooks, and corpora for evidence. By\n",
    "grounding claims in data, fact-checking makes LLMs more reliable.\n",
    "Pre-trained LLMs contain extensive world knowledge from their training data. Starting with the\n",
    "24-layer BERT-Large in 2018, language models have been pre-trained on large knowledge bases\n",
    "such as Wikipedia; therefore, they would be able to answer knowledge questions from Wikipedia\n",
    "or – since their training set increasingly includes other sources – the internet, textbooks, arXiv,\n",
    "and GitHub.\n",
    "We can prompt them with masking and other techniques to retrieve facts for evidence. For example, to answer the question “Where is Microsoft’s headquarters located?”, the question would be\n",
    "rewritten as “Microsoft’s headquarters is in [MASK]” and fed into a language model for the answer.\n",
    "Alternatively, we can integrate external tools to search knowledge bases, Wikipedia, textbooks,\n",
    "and other corpora. The key idea is verifying hallucinated claims by grounding them in factual\n",
    "data sources.\n",
    "Automatic fact-checking provides a way to make LLMs more reliable by checking that their responses align with real-world evidence. In the next sections, we’ll demonstrate this approach.\n",
    "In LangChain, we have a chain available for fact-checking with prompt chaining, where a model actively questions the assumptions that went into a statement. In this self-checking chain,\n",
    "LLMCheckerChain, the model is prompted sequentially – first, to make the assumptions explicit,\n",
    "which looks like this:\n",
    "Here's a statement: {statement}\\nMake a bullet point list of the\n",
    "assumptions you made when producing the above statement.\\n\n",
    "Please note that this is a string template, where the elements in curly brackets will be replaced\n",
    "by variables. Next, these assumptions are fed back to the model in order to check them one by\n",
    "one with a prompt like this:\n",
    "Here is a bullet point list of assertions:\n",
    "{assertions}\n",
    "For each assertion, determine whether it is true or false. If it is\n",
    "false, explain why.\\n\\n\n",
    "Finally, the model is tasked to make a final judgment:\n",
    "In light of the above facts, how would you answer the question\n",
    "'{question}'\n",
    "102 Building Capable Assistants\n",
    "LLMCheckerChain does this all by itself, as this example shows:\n",
    "from langchain.chains import LLMCheckerChain\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.7)\n",
    "text = \"What type of mammal lays the biggest eggs?\"\n",
    "checker_chain = LLMCheckerChain.from_llm(llm, verbose=True)\n",
    "checker_chain.run(text)\n",
    "The model can return different results to this question, some of which are wrong, and some of\n",
    "which it would correctly identify as false. When I was trying this out, I got results such as the blue\n",
    "whale, the North American beaver, and the extinct Giant Moa in response to my question \"What\n",
    "type of mammal lays the biggest eggs?\". The following is the right answer:\n",
    "Monotremes, a type of mammal found in Australia and parts of New Guinea,\n",
    "lay the largest eggs in the mammalian world. The eggs of the American\n",
    "echidna (spiny anteater) can grow as large as 10 cm in length, and\n",
    "dunnarts (mouse-sized marsupials found in Australia) can have eggs that\n",
    "exceed 5 cm in length.\n",
    "• Monotremes can be found in Australia and New Guinea\n",
    "• The largest eggs in the mammalian world are laid by monotremes\n",
    "• The American echidna lays eggs that can grow to 10 cm in length\n",
    "• Dunnarts lay eggs that can exceed 5 cm in length\n",
    "• Monotremes can be found in Australia and New Guinea – True\n",
    "• The largest eggs in the mammalian world are laid by monotremes – True\n",
    "• The American echidna lays eggs that can grow to 10 cm in length – False,\n",
    "the American echidna lays eggs that are usually between 1 to 4 cm in\n",
    "length.\n",
    "• Dunnarts lay eggs that can exceed 5 cm in length – False, dunnarts lay\n",
    "eggs that are typically between 2 to 3 cm in length.\n",
    "The largest eggs in the mammalian world are laid by monotremes, which can\n",
    "be found in Australia and New Guinea. Monotreme eggs can grow to 10 cm in\n",
    "length.\n",
    "> Finished chain.\n",
    "Chapter 4 103\n",
    "So, while this technique does not guarantee correct answers, it can put a stop to some incorrect\n",
    "results. Fact-checking approaches involve decomposing claims into smaller checkable queries,\n",
    "which can be formulated as question-answering tasks. Tools designed for searching domain\n",
    "datasets can assist fact-checkers in finding evidence effectively. Off-the-shelf search engines\n",
    "like Google and Bing can also retrieve both topically and evidentially relevant content to capture\n",
    "the veracity of a statement accurately. We’ll apply this approach to return results based on web\n",
    "searches and other applications of this chapter.\n",
    "In the next section, we’ll discuss automating the process of summarizing texts and longer documents such as research papers.\n",
    "\"\"\"\n",
    "\n",
    "to_markdown(chain.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stoic_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
